#+title: Tezos Context Lima Performance Audit
#+author: Irmin Team
#+STARTUP: inlineimages

* Introduction

The Octez ~tezos-context~ package maintains an on-disk representation of Tezos blocks. It is implemented using [[https://irmin.org/][Irmin]] and the purpose-built Irmin backend [[https://mirage.github.io/irmin/irmin-pack/index.html][~irmin-pack~]].

Performance of ~tezos-context~ is important for overall performance of Octez and previous improvments in ~irmin-pack~ have led to [[https://tarides.com/blog/2022-04-26-lightning-fast-with-irmin-tezos-storage-is-6x-faster-with-1000-tps-surpassed/][major performance improvements in Octez]].

This report contains an audit of the ~irmin-pack~ storage backend used by Tezos Octez. We investigate the performance of individual sub-components as well as access patterns used by Tezos.

** Overview of ~irmin-pack~

Irmin is a content-addressable data store similar to Git. Data is stored in trees where nodes and leaves of the trees are identified by their cryptographic hash. At it's core Irmin is a store of content-addressed objects (nodes and leaves).

~irmin-pack~ is a space-optimized Irmin backend inspired by Git packfiles that is currently used in Octez.

Conceptually ~irmin-pack~ stores content-addressed objects to an append-only file. A persistent datastructure called the index is used to map hashes to positions in the append-only file.

* Sub-components
** Index

The index is a persistent datastructure that maps the hash of an object to its offset in the append-only pack file.

Since Irmin version 3.0.0 objects in the pack file hold direct references to other objects in the pack file [[[https://github.com/mirage/irmin/pull/1659][irmin #1659]]] this removes the necessity to always query the index when traversing objects. This also allows a smaller index as now only top-level objects (i.e. commits) need to be in the index [[[https://github.com/mirage/irmin/pull/1664][irmin #1664]]]. This is called ~minimal~ indexing and has allowed [[https://tarides.com/blog/2022-04-26-lightning-fast-with-irmin-tezos-storage-is-6x-faster-with-1000-tps-surpassed][considerable performance improvements for the Octez storage layer]].

The index datastructure is split into two major parts: the log and the data. The log is a small and bounded structure that holds recently-added bindings. The log is kept in memory after initialization. The data part holds older bindings. When a certain number of bindings are added to the log, the bindings are merged with the bindings in the data part. The datastructure is similar to a two-level [[https://en.wikipedia.org/wiki/Log-structured_merge-tree][Log-structured merge-tree]].

The default number of bindings to keep in the log before moving them to the data part [[https://gitlab.com/tezos/tezos/-/blob/master/src/lib_context/helpers/env.ml#L41-45][in Octez]] and [[https://github.com/mirage/irmin/blob/main/src/irmin-pack/conf.mli#L93-L94][in ~irmin-pack~]] is set to 2_500_000.

*** Rolling and full node

We observe that rolling and full nodes [[https://tezos.gitlab.io/user/history_modes.html#history-mode-additional-cycles][keep a default of 6 cycles]] which corresponds to about 98_304 blocks ([[https://tezos.gitlab.io/active/proof_of_stake.html#ps-constants][16_834 blocks per cycle]]) or, in Irmin terminology, to about 98_304 commits. Rolling and full nodes will by default never create a data part and every index lookup performed is an in-memory operation.

A quick check confirms this:

#+begin_src shell :exports both
  dune exec audits/index/count_bindings.exe -- inputs/store-level-3081990/
#+end_src

#+RESULTS:
: Number of bindings in Index: 100005

Note that the number of bindings in the index is slightly higher than 98_304 as the store also contains orphaned commits.

*** Archive node

Tezos archive nodes hold the full history since genesis. When using minimal indexing references to at most 2_500_000 commits/blocks are kept in the log of the index. In July 2022 the Tezos block chain reached that block height. So since at least July 2022 the index of archive nodes will also have a ~data~ part.

Archive nodes that were bootstrapped before the new minimal indexing strategy was adopted have many more entries in the index:

#+begin_src shell :exports both
  dune exec audits/index/count_bindings.exe -- inputs/archive-node-index/
#+end_src

#+RESULTS:
: Number of bindings in Index: 2036584177


Archive nodes bootstrapped before the new minimal indexing strategy was adopted use around 90GiB of space just for the index structure.

*** Find Performance

We measure the performance of index lookups by creating an empty index structure with same parameters as used in Tezos Octez (key size of 30 bytes, value size of 13 bytes and log size of 2_500_000), adding ~c~ random bindings and performing ~c~ lookups for random bindings. We use the [[https://github.com/LexiFi/landmarks][LexiFi/landmarks]] library to measure performance in CPU cycles (as well as system time).

#+tblname: find-performance
| ^ |   count |           cpu_time |  sys_time | cpu_time per entry |
|---+---------+--------------------+-----------+--------------------|
| # |  250000 |  2132773826.000000 |  1.126439 |          8531.0953 |
| # |  500000 |  4009158441.000000 |  2.119049 |          8018.3169 |
| # | 1000000 |  8235106179.000000 |  4.351689 |          8235.1062 |
| # | 2000000 | 16838605030.000000 |  8.893822 |          8419.3025 |
| # | 2499999 | 20421445765.000000 | 10.791509 |          8168.5816 |
| # | 2500001 | 23511451504.000000 | 12.446721 |          9404.5768 |
| # | 3000000 | 31077192851.000000 | 16.442429 |          10359.064 |
| # | 4000000 | 39358430251.000000 | 20.823590 |          9839.6076 |
| # | 5000000 | 48122118368.000000 | 25.448949 |          9624.4237 |
| # | 6000000 | 60941841080.000000 | 32.247097 |          10156.974 |
| # | 7000000 | 72898690458.000000 | 38.564382 |          10414.099 |
#+TBLFM: $5=$3/$2

#+begin_src gnuplot :var data=find-performance :exports results :file find-performance.png
  reset

  set title "Index.find performance"

  set xlabel "number of entries"
  set format x '%.0f'

  set arrow from 2500000, graph 0 to 2500000, graph 1 nohead lc 2 title "log size"

  set ylabel "CPU cycles"

  plot data u 1:4 with point lw 2 title 'CPU cycles per entry'
#+end_src

#+RESULTS:
[[file:find-performance.png]]

We note a sharp increase in CPU cycles needed to lookup an entry when the number of bindings jumps over the log size (2_500_000). The lookup performance stays relatively constant at a higher level for up to 7_000_000 entries.

*** Conclusion

With the currently implemented minimal indexing scheme no performance issues are expected when using the default Tezos Octez configurations. For nodes running in history modes "rolling" and "full" the index is an in-memory structure. For archive nodes, no considerable performance degradation is expected up to at least block level 7_000_000.

Archive nodes that were bootstrapped using non-minimal indexing have a very large index structure. For better disk-usage it is recommended to re-bootstrap these nodes using minimal indexing.

** TODO Dict
* TODO IO access

See [[./audits/io/README.org]].

* Context structure and access patterns
** Content Size distribution

#+tblname: context-content-size
| Exponent |    Count |
|----------+----------|
|        0 |   596661 |
|        1 | 23722650 |
|        2 | 38698770 |
|        3 |  1125580 |
|        4 |  3131048 |
|        5 |  4194650 |
|        6 |  7477058 |
|        7 |   532486 |
|        8 |   194262 |
|        9 |    35410 |
|       10 |    35600 |
|       11 |    71535 |
|       12 |     4289 |
|       13 |     2583 |
|       14 |      260 |
|       15 |        9 |
|       16 |        1 |
|       17 |        3 |
|       18 |        7 |

#+begin_src gnuplot :var data=context-content-size :exports results :file context-content-size.png
  reset

  set title "Context Content Size"
  set style data histogram

  plot data using 2:xticlabels(1)
#+end_src

#+RESULTS:
[[file:context-content-size.png]]

* Modern hardware and asynchronous APIs
** TODO CPU boundness
* Conclusion
