#+title: Tezos Context Lima Performance Audit
#+date: June 2023
#+author: Irmin Team
#+STARTUP: inlineimages

* Introduction

The Octez ~tezos-context~ package maintains an on-disk representation of Tezos blocks. It is implemented using [[https://irmin.org/][Irmin]] and the purpose-built Irmin backend [[https://mirage.github.io/irmin/irmin-pack/index.html][~irmin-pack~]].

Performance of ~tezos-context~ is important for overall performance of Octez and previous improvments in ~irmin-pack~ have led to [[https://tarides.com/blog/2022-04-26-lightning-fast-with-irmin-tezos-storage-is-6x-faster-with-1000-tps-surpassed/][major performance improvements in Octez]].

This report contains an audit of the ~irmin-pack~ storage backend used by Tezos Octez. We investigate the performance of individual sub-components as well as access patterns used by Tezos.

** Overview of ~irmin-pack~

Irmin is a content-addressable data store similar to Git. Data is stored in trees where nodes and leaves of the trees are identified by their cryptographic hash. At it's core Irmin is a store of content-addressed objects (nodes and leaves).

~irmin-pack~ is a space-optimized Irmin backend inspired by Git packfiles that is currently used in Octez.

Conceptually ~irmin-pack~ stores content-addressed objects to an append-only file. A persistent datastructure called the index is used to map hashes to positions in the append-only file.

** Methodology

If not noted all tests were run on StarLabs Starbook Mk VI with a AMD Ryzen 7 5800U and a PCIe-3 connected 960GiB SSD.

For replays a recording of 100000 blocks starting at block level 2981990 was used.

We use the [[https://fio.readthedocs.io/][fio]] tool to simulate disk access patterns and measure performance. A baseline, based on observed access patterns of ~irmin-pack~ is provided in section [[*Baseline ~fio~ job]]

* Sub-components
** Index

The index is a persistent datastructure that maps the hash of an object to its offset in the append-only pack file.

Since Irmin version 3.0.0 objects in the pack file hold direct references to other objects in the pack file [[[https://github.com/mirage/irmin/pull/1659][irmin #1659]]] this removes the necessity to always query the index when traversing objects. This also allows a smaller index as now only top-level objects (i.e. commits) need to be in the index [[[https://github.com/mirage/irmin/pull/1664][irmin #1664]]]. This is called ~minimal~ indexing and has allowed [[https://tarides.com/blog/2022-04-26-lightning-fast-with-irmin-tezos-storage-is-6x-faster-with-1000-tps-surpassed][considerable performance improvements for the Octez storage layer]].

The index datastructure is split into two major parts: the log and the data. The log is a small and bounded structure that holds recently-added bindings. The log is kept in memory after initialization. The data part holds older bindings. When a certain number of bindings are added to the log, the bindings are merged with the bindings in the data part. The datastructure is similar to a two-level [[https://en.wikipedia.org/wiki/Log-structured_merge-tree][Log-structured merge-tree]].

The default number of bindings to keep in the log before moving them to the data part [[https://gitlab.com/tezos/tezos/-/blob/master/src/lib_context/helpers/env.ml#L41-45][in Octez]] and [[https://github.com/mirage/irmin/blob/main/src/irmin-pack/conf.mli#L93-L94][in ~irmin-pack~]] is set to 2500000.

*** Rolling and full node

We observe that rolling and full nodes [[https://tezos.gitlab.io/user/history_modes.html#history-mode-additional-cycles][keep a default of 6 cycles]] which corresponds to about 98304 blocks ([[https://tezos.gitlab.io/active/proof_of_stake.html#ps-constants][16_834 blocks per cycle]]) or, in Irmin terminology, to about 98304 commits. Rolling and full nodes will by default never create a data part and every index lookup performed is an in-memory operation.

A quick check confirms this:

#+begin_src shell :exports both
  dune exec audits/index/count_bindings.exe -- inputs/store-level-3081990/
#+end_src

#+RESULTS:
: Number of bindings in Index: 100005

Note that the number of bindings in the index is slightly higher than 98304 as the store also contains orphaned commits.

*** Archive node

Tezos archive nodes hold the full history since genesis. When using minimal indexing references to at most 2500000 commits/blocks are kept in the log of the index. In July 2022 the Tezos block chain reached that block height. So since at least July 2022 the index of archive nodes will also have a ~data~ part.

Archive nodes that were bootstrapped before the new minimal indexing strategy was adopted have many more entries in the index:

#+begin_src shell :exports both
  dune exec audits/index/count_bindings.exe -- inputs/archive-node-index/
#+end_src

#+RESULTS:
: Number of bindings in Index: 2036584177


Archive nodes bootstrapped before the new minimal indexing strategy was adopted use around 90GiB of space just for the index structure.

*** Find Performance

We measure the performance of index lookups by creating an empty index structure with same parameters as used in Tezos Octez (key size of 30 bytes, value size of 13 bytes and log size of 2500000), adding ~c~ random bindings and performing ~c~ lookups for random bindings. We use the [[https://github.com/LexiFi/landmarks][LexiFi/landmarks]] library to measure performance in CPU cycles (as well as system time).

#+tblname: find-performance
|   count |           cpu time |  sys time | cpu time per entry |
|---------+--------------------+-----------+--------------------|
|  250000 |  2132773826.000000 |  1.126439 |          8531.0953 |
|  500000 |  4009158441.000000 |  2.119049 |          8018.3169 |
| 1000000 |  8235106179.000000 |  4.351689 |          8235.1062 |
| 2000000 | 16838605030.000000 |  8.893822 |          8419.3025 |
| 2499999 | 20421445765.000000 | 10.791509 |          8168.5816 |
| 2500001 | 23511451504.000000 | 12.446721 |          9404.5768 |
| 3000000 | 31077192851.000000 | 16.442429 |          10359.064 |
| 4000000 | 39358430251.000000 | 20.823590 |          9839.6076 |
| 5000000 | 48122118368.000000 | 25.448949 |          9624.4237 |
| 6000000 | 60941841080.000000 | 32.247097 |          10156.974 |
| 7000000 | 72898690458.000000 | 38.564382 |          10414.099 |

#+begin_src gnuplot :var data=find-performance :exports results :file find-performance.png
  reset

  set title "Index.find performance"

  set xlabel "number of entries"
  set format x '%.0f'

  set arrow from 2500000, graph 0 to 2500000, graph 1 nohead lc 2 title "log size"

  set ylabel "CPU cycles"

  plot data u 1:4 with point lw 2 title 'CPU cycles per entry'
#+end_src

#+RESULTS:
[[file:find-performance.png]]

We note a sharp increase in CPU cycles needed to lookup an entry when the number of bindings jumps over the log size (2500000). The lookup performance stays relatively constant at a higher level for up to 7000000 entries.

*** Conclusion

With the currently implemented minimal indexing scheme no performance issues are expected when using the default Tezos Octez configurations. For nodes running in history modes "rolling" and "full" the index is an in-memory structure. For archive nodes, no considerable performance degradation is expected up to at least block level 7000000.

Archive nodes that were bootstrapped using non-minimal indexing have a very large index structure. For better disk-usage it is recommended to re-bootstrap these nodes using minimal indexing.

** Dict

Irmin stores values in a tree where paths are sequences of strings. In order to de-duplicate commonly occuring path segments, some path segments are stored in an auxilary persistent structure called the ~Dict~.

The ~Dict~ maintains a mapping from path segments (strings) to integer identifiers. In the Irmin tree the integer identifiers can be used instead of the string path segment. As integer identifiers are in general smaller than the string path segments this results in a more compact on-disk representation.

In ~irmin-pack~ the ~Dict~ is limited to 100000 entries that were added on a first-come-first-serve basis. The ~Dict~ is currently full.

*** Occurrences of Dict elements in store

We count the number of occurrences of path segments that appear in the ~Dict~ in the store of the Tezos block 3081990.

It seems like the some common path segments are de-duplicated a considerable amount of times:

| Path segment                                                     | Occurrences in store |
|------------------------------------------------------------------+----------------------|
| 4a1cf11667fa0165eac9963333b883a80bcfdfebde09b79bfc740680e986bab6 |               108903 |
| 053f610929e2b6ea458c54dfd8b29716d379c13f5c8fd82d5c793a9e31271743 |                90893 |
| 00d0158265571a474bc6ed02547db51416ab2228327e66332117ea7b587aca94 |                13912 |
| 1ffdaf1cd7574b72f933a9d5e102143f3e4d761a6e51b4019ed821b7b99b097a |                 2313 |
| 04034e4a6228fdb92e8978fb85d9c2d1f79501b0c509f24b0f1eede3ca7cb234 |                 1611 |
| 10f21b2eacdf858cf9824d29e9c0d09bf666d3d900fbc54b6438f67e63831d4d |                 1157 |
| 2966fdb0cb953d94e959dcee2b2c3238c42bf0d1e0991a5a51609059aaa04080 |                  890 |
| 1f33bf814d191cc602888479ef371d13082f19718f63737e685cc76110e323d9 |                  813 |
| 02e5f95f2c3a3ccfa5ce71d0f11ad70f4746b8e0f3fe7bcecc63dbc8cfba71d1 |                  731 |
| 438c52065d4605460b12d1b9446876a1c922b416103a20d44e994a9fd2b8ed07 |                  477 |
| 00642bcad8681caf0f45f195cc1483f8366f155d83e272c9ff93fe3840a61dcb |                  396 |
| 4001852857ca1c5dad1c1275f766fc5208e63c48ba0289127591ffef3c440d53 |                  388 |
| 27e1640238d07d569852b3e4fe784f5adce0e6649673ea587aa7389d72b855af |                  381 |

However we also note that most entries in the ~Dict~ do not occur in the store. We count 96394 ~Dict~ entries that do not occur in the store.

Furthermore, we observe that most entries in the ~Dict~ are hashes. Commonly used short keywords like ~total_bytes~, ~missed_endorsement~ or ~index~ are not ~Dict~ entries.

*** Conclussion

We conclude that the ~Dict~ is underused. This observation has [[https://github.com/mirage/irmin/issues/1807][been previously made]]. A more efficient use of the ~Dict~ could lead to a considerably more compact on-disk representation and potential performance improvements. Note that existing ~Dict~ entries can not be removed and it might be tricky to get a performant ~Dict~ that is considerably larger. Furthermore, potential performance improvements seem hard to quanitfy.

We can not recommend any concrete measures for optimizing overall performance with improvements to the ~Dict~.

* IO Activity

In order to measure real disk IO accesses we add some [[https://github.com/mirage/irmin/pull/2250][instrumentation to irmin-pack]] that allows us to measure how many bytes are read/written to individual files in how many system calls. In addition we also trace calls to IO reads and writes using the [[https://github.com/LexiFi/landmarks][landmarks]] library.

|                                         |              Read |             Write |
|-----------------------------------------+-------------------+-------------------|
| Bytes in 100000 blocks                  | 59141777410 (56%) | 46354750320 (44%) |
| Bytes per block (mean)                  |        591394.118 |        463528.962 |
| Number of system calls per block (mean) |          9645.864 |             4.061 |
| Bytes per system call (mean)            |         61.310642 |         114141.58 |
| I/O time                                |             96.5% |              3.5% |

We observe:

- Total I/O activity in bytes consists of about 56% reads and 44% writes. However, reads take 96.5% of the time [fn:: We measure this by tracing the ~Irmin_pack_unix.Io.Util.really_read~ and ~Irmin_pack_unix.Io.Util.really_write~ functions].
- Reads are performed in many very small chunks (61 bytes per system call).

As read performance dominates the overall performance we concentrate on it in the following.

** Baseline ~fio~ job

Based on the observations we can create a baseline ~fio~ job description that simulates the read access pattern of ~irmin-pack~:

#+begin_src ini :tangle baseline-reads.ini
[global]
rw=randread
filename=/home/adatario/dev/tclpa/.git/annex/objects/gx/17/SHA256E-s3691765475--13300581f2404cc24774da8615a5a3d3f0adb7d68c4c8034c4fa69e727706000/SHA256E-s3691765475--13300581f2404cc24774da8615a5a3d3f0adb7d68c4c8034c4fa69e727706000

[job1]
ioengine=psync
rw=randread
blocksize_range=50-100
size=591394B
loops=100
#+end_src

#+begin_src shell :exports both :results output code
  fio baseline-reads.ini
#+end_src

#+RESULTS:
#+begin_src shell
job1: (g=0): rw=randread, bs=(R) 50B-100B, (W) 50B-100B, (T) 50B-100B, ioengine=psync, iodepth=1
fio-3.35
Starting 1 process

job1: (groupid=0, jobs=1): err= 0: pid=153129: Thu Jun 15 10:14:00 2023
  read: IOPS=328k, BW=21.0MiB/s (22.1MB/s)(56.4MiB/2681msec)
    clat (nsec): min=210, max=3742.4k, avg=2833.00, stdev=20598.76
     lat (nsec): min=230, max=3743.1k, avg=2858.85, stdev=20605.58
    clat percentiles (nsec):
     |  1.00th=[   211],  5.00th=[   221], 10.00th=[   221], 20.00th=[   221],
     | 30.00th=[   231], 40.00th=[   231], 50.00th=[   231], 60.00th=[   231],
     | 70.00th=[   241], 80.00th=[   402], 90.00th=[   410], 95.00th=[   676],
     | 99.00th=[150528], 99.50th=[158720], 99.90th=[191488], 99.95th=[230400],
     | 99.99th=[329728]
   bw (  KiB/s): min=20846, max=21922, per=99.73%, avg=21483.80, stdev=449.08, samples=5
   iops        : min=317386, max=333862, avg=327161.20, stdev=6851.62, samples=5
  lat (nsec)   : 250=70.87%, 500=23.40%, 750=1.39%, 1000=1.64%
  lat (usec)   : 2=0.93%, 4=0.08%, 10=0.06%, 20=0.01%, 50=0.01%
  lat (usec)   : 250=1.59%, 500=0.03%, 750=0.01%, 1000=0.01%
  lat (msec)   : 4=0.01%
  cpu          : usr=12.13%, sys=9.74%, ctx=14307, majf=0, minf=10
  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=879400,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=1

Run status group 0 (all jobs):
   READ: bw=21.0MiB/s (22.1MB/s), 21.0MiB/s-21.0MiB/s (22.1MB/s-22.1MB/s), io=56.4MiB (59.1MB), run=2681-2681msec

Disk stats (read/write):
    dm-1: ios=14145/0, merge=0/0, ticks=2116/0, in_queue=2116, util=96.36%, aggrios=14301/0, aggrmerge=0/0, aggrticks=2132/0, aggrin_queue=2132, aggrutil=95.57%
    dm-0: ios=14301/0, merge=0/0, ticks=2132/0, in_queue=2132, util=95.57%, aggrios=14301/0, aggrmerge=0/0, aggrticks=1539/0, aggrin_queue=1539, aggrutil=95.57%
  nvme0n1: ios=14301/0, merge=0/0, ticks=1539/0, in_queue=1539, util=95.57%
#+end_src

We observe a read band-width of 21.0MiB/s which seems to match our observations.

* TODO Compactness of on-disk representation

Having a compact on-disk representation is not only good for disk usage but can also improve overall performance as data can be loaded into memory closer to the processor faster if it is smaller.

We analyze the compactness of the on-disk representation of ~irmin-pack~ by compressing with the Zstandard compression algorithm. Compact representation have a low compression ratio, whereas less compact representations may admit a more compact representation (for example by compressing with Zstandard).

| Store                            | Uncompressed |  Compressed | Compression Ratio |
|----------------------------------+--------------+-------------+-------------------|
| Level 2981990                    |   5108531200 |  3265930512 |         1.5641886 |
| Level 3081990                    |  32111902720 | 10332988078 |         3.1077073 |
| Single suffix from level 2981990 |   5101987840 |  3261700639 |         1.5642109 |
| Single suffix from level 3081990 |   3633868800 |   940228651 |         3.8648778 |
#+TBLFM: $4=($2/$3)

** TODO Why is there such a difference between store of level 2981990 and 3081990

* Context structure and access patterns
** TODO Content Size distribution

- This might be a motivation for inlining small objects (see https://github.com/mirage/irmin/issues/884)

#+tblname: context-content-size
| Exponent (base 2) |    Count |
|-------------------+----------|
|                 0 |   596661 |
|                 1 | 23722650 |
|                 2 | 38698770 |
|                 3 |  1125580 |
|                 4 |  3131048 |
|                 5 |  4194650 |
|                 6 |  7477058 |
|                 7 |   532486 |
|                 8 |   194262 |
|                 9 |    35410 |
|                10 |    35600 |
|                11 |    71535 |
|                12 |     4289 |
|                13 |     2583 |
|                14 |      260 |
|                15 |        9 |
|                16 |        1 |
|                17 |        3 |
|                18 |        7 |

#+begin_src gnuplot :var data=context-content-size :exports results :file context-content-size.png
  reset

  set title "Context Content Size"
  set style data histogram

  set xlabel "Logarithm of size (base 2)"
  set ylabel "Count"

  plot data using 2:xticlabels(1) notitle
#+end_src

#+RESULTS:
[[file:context-content-size.png]]

** TODO Access Patterns
* CPU boundness

We run a replay of 100000 Tezos blocks with three different CPU frequencies:

- 3.40GHz: Run on an Equinix Metal `c3.small.x86` machine with an Intel® Xeon® E-2278G CPU with 8 cores running at 3.40 GHz.
- 2.5GHz: Run on an Equinix Metal `m3.large.x86` machine with an AMD EPYC 7502P CPU with 32 cores running at 2.5 GHz.
- 1.5GHz: Run on an Equinix Metal `m3.large.x86` machine with an AMD EPYC 7502P CPU with 32 cores at 1.5 GHz. The CPU frequency was set using `cpupower frequency-set 1.5GHz`.

| CPU Frequency           | 3.40GHz  | 2.5GHz        | 1.5GHz        |
| CPU time elapsed        | 73m27s   | 103m26s 141%  | 194m37s 265%  |
| Wall time elapsed       | 76m08s   | 103m24s 136%  | 194m28s 255%  |
| TZ-transactions per sec | 1043.919 | 741.288  71%  | 393.969  38%  |
| TZ-operations per sec   | 6818.645 | 4841.928  71% | 2573.316  38% |

This seems to indicate that ~irmin-pack~ is CPU-bound. Improving CPU efficiency should directly improve overall performance.

** Disable OS page cache

Operating systems implement their own cache mechanism for disk access. This [[https://db.in.tum.de/~leis/papers/leanstore.pdf][can cause additional CPU cycles]] which are unnecessary as ~irmin-pack.unix~ implements it's own cache.

We can disable the operating system to cache blocks by using the [[https://linux.die.net/man/2/fadvise][~fadvise~]] system call with ~FADV_NOREUSE~. This will prevent the operating system from maintaining blocks in cache, thus saving CPU cycles.

We can tell ~fio~ to do this with the ~fadvise_hint~ option:

#+begin_src ini :tangle fadvise-noreuse.ini
[global]
rw=randread
filename=/home/adatario/dev/tclpa/.git/annex/objects/gx/17/SHA256E-s3691765475--13300581f2404cc24774da8615a5a3d3f0adb7d68c4c8034c4fa69e727706000/SHA256E-s3691765475--13300581f2404cc24774da8615a5a3d3f0adb7d68c4c8034c4fa69e727706000

[job1]
ioengine=psync
rw=randread
blocksize_range=50-100
size=591394B
loops=100
fadvise_hint=noreuse
#+end_src

#+begin_src shell :exports both :results output code
  fio fadvise-noreuse.ini
#+end_src

#+RESULTS:
#+begin_src shell
job1: (g=0): rw=randread, bs=(R) 50B-100B, (W) 50B-100B, (T) 50B-100B, ioengine=psync, iodepth=1
fio-3.35
Starting 1 process

job1: (groupid=0, jobs=1): err= 0: pid=156889: Thu Jun 15 10:55:44 2023
  read: IOPS=432k, BW=27.7MiB/s (29.1MB/s)(56.4MiB/2035msec)
    clat (nsec): min=210, max=3652.8k, avg=2107.02, stdev=18068.27
     lat (nsec): min=230, max=3653.4k, avg=2131.96, stdev=18076.02
    clat percentiles (nsec):
     |  1.00th=[   211],  5.00th=[   221], 10.00th=[   221], 20.00th=[   221],
     | 30.00th=[   231], 40.00th=[   231], 50.00th=[   231], 60.00th=[   231],
     | 70.00th=[   241], 80.00th=[   402], 90.00th=[   410], 95.00th=[   442],
     | 99.00th=[121344], 99.50th=[152576], 99.90th=[224256], 99.95th=[254976],
     | 99.99th=[346112]
   bw (  KiB/s): min=27723, max=28868, per=99.72%, avg=28298.25, stdev=476.32, samples=4
   iops        : min=422174, max=439618, avg=430933.00, stdev=7326.27, samples=4
  lat (nsec)   : 250=71.97%, 500=24.01%, 750=1.26%, 1000=0.95%
  lat (usec)   : 2=0.51%, 4=0.07%, 10=0.06%, 20=0.01%, 50=0.02%
  lat (usec)   : 100=0.02%, 250=1.06%, 500=0.05%, 750=0.01%, 1000=0.01%
  lat (msec)   : 4=0.01%
  cpu          : usr=12.00%, sys=15.34%, ctx=10101, majf=0, minf=11
  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=879400,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=1

Run status group 0 (all jobs):
   READ: bw=27.7MiB/s (29.1MB/s), 27.7MiB/s-27.7MiB/s (29.1MB/s-29.1MB/s), io=56.4MiB (59.1MB), run=2035-2035msec

Disk stats (read/write):
    dm-1: ios=11489/0, merge=0/0, ticks=1868/0, in_queue=1868, util=94.89%, aggrios=12303/0, aggrmerge=0/0, aggrticks=2016/0, aggrin_queue=2016, aggrutil=94.15%
    dm-0: ios=12303/0, merge=0/0, ticks=2016/0, in_queue=2016, util=94.15%, aggrios=12303/0, aggrmerge=0/0, aggrticks=1629/0, aggrin_queue=1628, aggrutil=94.15%
  nvme0n1: ios=12303/0, merge=0/0, ticks=1629/0, in_queue=1628, util=94.15%
#+end_src

We observe a read bandwidth of 27.7MiB/s. This seems to be a considerable performance improvement (around 30%) that can be implemented fairly easily.

* Multicore

In order to simulate the potential performance improvement of using multiple cores we use a ~fio~ job similar to the baseline but instead use ~N~ cores that in total read the same amount of bytes:

#+begin_src ini :tangle multiple-threads-pread.ini
[global]
rw=randread
filename=/home/adatario/dev/tclpa/.git/annex/objects/gx/17/SHA256E-s3691765475--13300581f2404cc24774da8615a5a3d3f0adb7d68c4c8034c4fa69e727706000/SHA256E-s3691765475--13300581f2404cc24774da8615a5a3d3f0adb7d68c4c8034c4fa69e727706000
loops=100
group_reporting
thread

[job1]
ioengine=psync
rw=randread
blocksize_range=50-100
size=591394 / N
numjobs=N
#+end_src

Using the options ~thread~ and ~numjobs~ we create ~N~ threads that randomly read from the file.

We observe following read bandwidths:

#+tblname: multicore-read-bandwidth
| Number of Threads | Read bandwidth (MiB/s) |
|-------------------+------------------------|
|                 1 |                     21 |
|                 2 |                   58.8 |
|                 3 |                   89.4 |
|                 4 |                    109 |
|                 5 |                    120 |
|                 6 |                    141 |
|                 7 |                    148 |
|                 8 |                    162 |
|                 9 |                    168 |
|                10 |                    185 |
|                11 |                    196 |
|                12 |                    201 |
|                13 |                    213 |
|                14 |                    219 |
|                15 |                    228 |
|                16 |                    234 |

#+begin_src gnuplot :var data=multicore-read-bandwidth :exports results :file multicore-read-bandwidth.png

  reset

  set title "Multicore read bandwidth"
  set xlabel "Number of threads"
  set ylabel "Read bandwidth (MiB/s)"

  plot data with linespoints notitle

#+end_src

#+RESULTS:
[[file:multicore-read-bandwidth.png]]

Increasing the number of threads/CPU cores utilized seems to lead to considerable performance improvements.

Note however, that the we simulate ~N~ independent threads. In ~irmin-pack~, and Irmin in general, reads may need to be sequential as they descend a tree by de-referencing nodes individually.

* Modern hardware and asynchronous APIs

Modern PCIe-attached solid-state drives (SSDs) offer high throughput and
large capacity at low cost. They are exposed to the system using the same block-based APIs as traditional disks or SSDs attached via serial buses. However, in order to utilize the full performance of such modern hardware, the way I/O operations are performed requires rethinking.

We do some explorations on how performance could be improved using modern hardware and APIs using the [[https://fio.readthedocs.io/en/latest/fio_doc.html][fio]] tool.

** Asynchronous I/O

In order to illustrate the capabilities of modern hardware we run ~fio~ using the Linux [[https://en.wikipedia.org/wiki/Io_uring][io_uring]] API. This allows asynchronous I/O operations.

We also increase size of the blocks read from a few bytes to 64KiB. This increases latency for individual reads, but allows much higher bandwidth.

#+begin_src ini :tangle read-io_uring.ini
[global]
rw=randread
filename=/home/adatario/dev/tclpa/.git/annex/objects/gx/17/SHA256E-s3691765475--13300581f2404cc24774da8615a5a3d3f0adb7d68c4c8034c4fa69e727706000/SHA256E-s3691765475--13300581f2404cc24774da8615a5a3d3f0adb7d68c4c8034c4fa69e727706000
loops=100
group_reporting
thread

[job1]
ioengine=io_uring
iodepth=16
rw=randread
blocksize=64KiB
size=100MiB
numjobs=8
#+end_src

#+begin_src shell :exports both :results output code
  fio read-io_uring.ini
#+end_src

#+RESULTS:
#+begin_src shell
job1: (g=0): rw=randread, bs=(R) 62.5KiB-62.5KiB, (W) 62.5KiB-62.5KiB, (T) 62.5KiB-62.5KiB, ioengine=io_uring, iodepth=16
...
fio-3.33
Starting 8 threads

job1: (groupid=0, jobs=8): err= 0: pid=44365: Fri May 19 14:25:25 2023
  read: IOPS=147k, BW=8955MiB/s (9390MB/s)(74.5GiB/8517msec)
    slat (nsec): min=290, max=9700.4k, avg=9392.25, stdev=73806.67
    clat (nsec): min=130, max=23784k, avg=767189.99, stdev=1005905.48
     lat (usec): min=4, max=23786, avg=776.58, stdev=1007.13
    clat percentiles (usec):
     |  1.00th=[   17],  5.00th=[   30], 10.00th=[   43], 20.00th=[   62],
     | 30.00th=[   83], 40.00th=[  125], 50.00th=[  215], 60.00th=[  400],
     | 70.00th=[  914], 80.00th=[ 1795], 90.00th=[ 2278], 95.00th=[ 2606],
     | 99.00th=[ 3884], 99.50th=[ 4490], 99.90th=[ 6390], 99.95th=[ 7439],
     | 99.99th=[10028]
   bw (  MiB/s): min= 7688, max=10172, per=100.00%, avg=9047.06, stdev=83.12, samples=129
   iops        : min=125968, max=166674, avg=148226.72, stdev=1361.83, samples=129
  lat (nsec)   : 250=0.01%, 500=0.01%, 750=0.01%, 1000=0.01%
  lat (usec)   : 2=0.01%, 4=0.02%, 10=0.10%, 20=1.72%, 50=11.80%
  lat (usec)   : 100=21.71%, 250=17.36%, 500=10.21%, 750=4.71%, 1000=3.36%
  lat (msec)   : 2=12.41%, 4=15.67%, 10=0.88%, 20=0.01%, 50=0.01%
  cpu          : usr=2.68%, sys=14.62%, ctx=559563, majf=0, minf=0
  IO depths    : 1=0.1%, 2=0.1%, 4=0.3%, 8=0.5%, 16=99.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=99.9%, 8=0.0%, 16=0.1%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=1249600,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=16

Run status group 0 (all jobs):
   READ: bw=8955MiB/s (9390MB/s), 8955MiB/s-8955MiB/s (9390MB/s-9390MB/s), io=74.5GiB (80.0GB), run=8517-8517msec

Disk stats (read/write):
    dm-1: ios=345735/95, merge=0/0, ticks=597292/4, in_queue=597296, util=98.72%, aggrios=349972/95, aggrmerge=0/0, aggrticks=599656/4, aggrin_queue=599660, aggrutil=98.56%
    dm-0: ios=349972/95, merge=0/0, ticks=599656/4, in_queue=599660, util=98.56%, aggrios=349972/87, aggrmerge=0/8, aggrticks=542294/6, aggrin_queue=542303, aggrutil=97.77%
  nvme0n1: ios=349972/87, merge=0/8, ticks=542294/6, in_queue=542303, util=97.77%
#+end_src

We observe a read bandwidth of about 9GiB/s which illustrates the capabilities of modern hardware.

* TODO Conclusion

Based on the audits performed we can propose following improvements and changes to the Octez storage backend:

| Description                                                     | Estimated improvement | Effort    |
|-----------------------------------------------------------------+-----------------------+-----------|
| [[*Disable OS page cache]]                                          | 30%                   | Low       |
| Use multiple read threads (see [[*Multicore]])                      | 700%                  | High      |
| General CPU efficiency improvements (see [[*CPU boundness]]         | unknown               | Medium    |
| More compact on-disk representation                             | unknown               | High      |
| TODO Inline small objects                                       | unknown               | High      |
| TODO Optimize Tezos usage patterns                              | unknown               | High      |
| Asynchrounous I/O with larger block size (see [[Asynchronous I/O]]) | 30000%                | Very high |

