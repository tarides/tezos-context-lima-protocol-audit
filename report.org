#+title: Tezos Context Lima Performance Audit
#+author: Irmin Team

#+BEGIN_abstract
#+END_abstract

* Introduction

* Audits

** TODO Dict usage

** TODO Index performance

At it's core Irmin is a content-addressable store. Store objects are referenced by their cryptographic hash. In ~irmin-pack~ objects are stored in an append-only pack file. The index is a persistent datastructure that maps the hash of an object to its offset in the append-only pack file.

Since Irmin version 3.0.0 objects in the pack file hold direct references to other objects in the pack file [[[https://github.com/mirage/irmin/pull/1659][irmin #1659]]] this removes the necessity to always query the index when traversing objects. This also allows a smaller index as now only top-level objects (i.e. commits) need to be in the index [[[https://github.com/mirage/irmin/pull/1664][irmin #1664]]]. This is called ~minimal~ indexing and has allowed [[https://tarides.com/blog/2022-04-26-lightning-fast-with-irmin-tezos-storage-is-6x-faster-with-1000-tps-surpassed][considerable performance improvements for the Octez storage layer]].

The index datastructure is split into two major parts: the log and the data. The log is a small and bounded structure that holds recently-added bindings. The log is kept in memory after initialization. The data part holds older bindings. When a certain number of bindings are added to the log, the bindings are merged with the bindings in the data part. The datastructure is similar to a two-level [[https://en.wikipedia.org/wiki/Log-structured_merge-tree][Log-structured merge-tree]].

The default number of bindings to keep in the log before moving them to the data part [[https://gitlab.com/tezos/tezos/-/blob/master/src/lib_context/helpers/env.ml#L41-45][in Octez]] and [[https://github.com/mirage/irmin/blob/main/src/irmin-pack/conf.mli#L93-L94][in ~irmin-pack~]] is set to 2_500_000.

*** Rolling and full node

We observe that rolling and full nodes [[https://tezos.gitlab.io/user/history_modes.html#history-mode-additional-cycles][keep a default of 6 cycles]] which corresponds to about 98_304 blocks ([[https://tezos.gitlab.io/active/proof_of_stake.html#ps-constants][16_834 blocks per cycle]]) or, in Irmin terminology, to about 98_304 commits. Rolling and full nodes will by default never create a data part and every index lookup performed is an in-memory operation.

A quick check confirms this:

#+begin_src shell :exports both
  dune exec audits/index/count_bindings.exe -- inputs/store-level-3081990/
#+end_src

#+RESULTS:
: Number of bindings in Index: 100005

Note that the number of bindings in the index is slightly higher than 98_304 as the store also contains orphaned commits.

*** Archive node

Tezos archive nodes hold the full history since genesis. When using minimal indexing references to at most 2_500_000 commits/blocks are kept in the log of the index. In July 2022 the Tezos block chain reached that block height. So since at least July 2022 the index of archive nodes will also have a ~data~ part.

Archive nodes that were bootstrapped before the new minimal indexing strategy was adopted have many more entries in the index:

#+begin_src shell :exports both
  dune exec audits/index/count_bindings.exe -- inputs/archive-node-index/
#+end_src

#+RESULTS:
: Number of bindings in Index: 2036584177

*** Find Performance

We measure the performance of index lookups by creating an empty index structure with same parameters as used in Tezos Octez (key size of 30 bytes, value size of 13 bytes and log size of 2_500_000), adding ~c~ random bindings and performing ~c~ lookups for random bindings. We use the [[https://github.com/LexiFi/landmarks][LexiFi/landmarks]] library to measure performance in CPU cycles (as well as system time).

#+tblname: find-performance
| ^ |   count |           cpu_time |  sys_time | cpu_time per entry |
|---+---------+--------------------+-----------+--------------------|
| # |  250000 |  2132773826.000000 |  1.126439 |          8531.0953 |
| # |  500000 |  4009158441.000000 |  2.119049 |          8018.3169 |
| # | 1000000 |  8235106179.000000 |  4.351689 |          8235.1062 |
| # | 2000000 | 16838605030.000000 |  8.893822 |          8419.3025 |
| # | 2499999 | 20421445765.000000 | 10.791509 |          8168.5816 |
| # | 2500001 | 23511451504.000000 | 12.446721 |          9404.5768 |
| # | 3000000 | 31077192851.000000 | 16.442429 |          10359.064 |
| # | 4000000 | 39358430251.000000 | 20.823590 |          9839.6076 |
| # | 5000000 | 48122118368.000000 | 25.448949 |          9624.4237 |
| # | 6000000 | 60941841080.000000 | 32.247097 |          10156.974 |
| # | 7000000 | 72898690458.000000 | 38.564382 |          10414.099 |
#+TBLFM: $5=$3/$2

#+begin_src gnuplot :var data=find-performance :exports code :file find-performance.png
  set title "Index.find performance"

  set xlabel "number of entries"
  set format x '%.0f'

  set arrow from 2500000, graph 0 to 2500000, graph 1 nohead lc 2 title "log size"

  set ylabel "CPU cycles"

  plot data u 1:4 with point lw 2 title 'CPU cycles per entry'
#+end_src

#+RESULTS:
[[file:find-performance.png]]

We note a sharp increase in CPU cycles needed to lookup an entry when the number of bindings jumps over the log size (2_500_000).

** TODO I/O Amplification

** TODO CPU boundness

** TODO Entropy of Store
