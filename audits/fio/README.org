#+title: Use fio to simulate ~irmin-pack.unix~ load

* Read performance
** Baseline ~irmin-pack.unix~ reads

We attempt to simulate a similar behaviour as is currently implemented by ~irmin-pack.unix~.

From observations we have that:

- During the processing of a single Tezos Block about 3.5 MiB is read from disk
- Average access size is between 50-100 bytes
- ~irmin-pack.unix~ uses the `pread` system calls from a single thread

#+begin_src ini :tangle baseline-reads.ini
[global]
rw=randread
filename=/home/adatario/dev/tclpa/.git/annex/objects/gx/17/SHA256E-s3691765475--13300581f2404cc24774da8615a5a3d3f0adb7d68c4c8034c4fa69e727706000/SHA256E-s3691765475--13300581f2404cc24774da8615a5a3d3f0adb7d68c4c8034c4fa69e727706000

[job1]
ioengine=psync
rw=randread
blocksize_range=50-300
size=3500000B
loops=100
#+end_src

#+begin_src shell :exports both :results output code
  fio baseline-reads.ini
#+end_src

#+RESULTS:
#+begin_src shell
job1: (g=0): rw=randread, bs=(R) 50B-300B, (W) 50B-300B, (T) 50B-300B, ioengine=psync, iodepth=1
fio-3.33
Starting 1 process

job1: (groupid=0, jobs=1): err= 0: pid=42484: Fri May 19 14:18:35 2023
  read: IOPS=230k, BW=29.8MiB/s (31.2MB/s)(334MiB/11209msec)
    clat (nsec): min=210, max=4069.4k, avg=4129.23, stdev=21131.34
     lat (nsec): min=230, max=4070.1k, avg=4154.08, stdev=21135.27
    clat percentiles (nsec):
     |  1.00th=[   221],  5.00th=[   231], 10.00th=[   231], 20.00th=[   231],
     | 30.00th=[   241], 40.00th=[   241], 50.00th=[   241], 60.00th=[   251],
     | 70.00th=[   310], 80.00th=[   330], 90.00th=[   362], 95.00th=[   612],
     | 99.00th=[123392], 99.50th=[130560], 99.90th=[136192], 99.95th=[136192],
     | 99.99th=[148480]
   bw (  KiB/s): min=27526, max=33845, per=99.84%, avg=30444.41, stdev=2900.78, samples=22
   iops        : min=207124, max=255516, avg=229381.59, stdev=22386.75, samples=22
  lat (nsec)   : 250=55.59%, 500=37.79%, 750=2.44%, 1000=0.65%
  lat (usec)   : 2=0.13%, 4=0.13%, 10=0.01%, 20=0.01%, 100=0.05%
  lat (usec)   : 250=3.23%, 500=0.01%, 750=0.01%
  lat (msec)   : 10=0.01%
  cpu          : usr=7.52%, sys=8.17%, ctx=84419, majf=0, minf=12
  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=2575300,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=1

Run status group 0 (all jobs):
   READ: bw=29.8MiB/s (31.2MB/s), 29.8MiB/s-29.8MiB/s (31.2MB/s-31.2MB/s), io=334MiB (350MB), run=11209-11209msec

Disk stats (read/write):
    dm-1: ios=84008/120, merge=0/0, ticks=9396/0, in_queue=9396, util=94.84%, aggrios=84402/120, aggrmerge=0/0, aggrticks=9400/0, aggrin_queue=9400, aggrutil=94.70%
    dm-0: ios=84402/120, merge=0/0, ticks=9400/0, in_queue=9400, util=94.70%, aggrios=84402/99, aggrmerge=0/21, aggrticks=8827/5, aggrin_queue=8833, aggrutil=94.70%
  nvme0n1: ios=84402/99, merge=0/21, ticks=8827/5, in_queue=8833, util=94.70%
#+end_src

A read bandwidth of about 30MiB/s. This seems to match up with the read performance observed when replaying a Tezos trace.

** Multiple Threads

Simulate multiple threads doing IO reads.

| Number of Threads | Read bandwidth (MiB/s) |
|-------------------+------------------------|
|                 1 |                   23.4 |
|                 2 |                   72.8 |
|                 3 |                    102 |
|                 4 |                    127 |
|                 5 |                    147 |
|                 6 |                    163 |
|                 7 |                    175 |
|                 8 |                    167 |
|                 9 |                    175 |
|                10 |                    172 |

TODO: write a script that does this automatically. For some reason I can't get division working in the fio job description file (https://fio.readthedocs.io/en/latest/fio_doc.html#job-file-parameters).

#+begin_src ini :tangle multiple-threads-pread.ini
[global]
rw=randread
filename=/home/adatario/dev/tclpa/.git/annex/objects/gx/17/SHA256E-s3691765475--13300581f2404cc24774da8615a5a3d3f0adb7d68c4c8034c4fa69e727706000/SHA256E-s3691765475--13300581f2404cc24774da8615a5a3d3f0adb7d68c4c8034c4fa69e727706000
loops=100
group_reporting
thread

[job1]
ioengine=psync
rw=randread
blocksize_range=50-300
size=350000
numjobs=10
#+end_src

#+begin_src shell :exports both :results output code
  fio multiple-threads-pread.ini
#+end_src

#+RESULTS:
#+begin_src shell
job1: (g=0): rw=randread, bs=(R) 50B-300B, (W) 50B-300B, (T) 50B-300B, ioengine=psync, iodepth=1
...
fio-3.33
Starting 10 threads

job1: (groupid=0, jobs=10): err= 0: pid=42511: Fri May 19 14:19:24 2023
  read: IOPS=1310k, BW=170MiB/s (179MB/s)(334MiB/1958msec)
    clat (nsec): min=210, max=4039.1k, avg=7158.23, stdev=39168.20
     lat (nsec): min=240, max=4040.0k, avg=7193.81, stdev=39168.62
    clat percentiles (nsec):
     |  1.00th=[   241],  5.00th=[   262], 10.00th=[   270], 20.00th=[   310],
     | 30.00th=[   322], 40.00th=[   342], 50.00th=[   382], 60.00th=[   442],
     | 70.00th=[   482], 80.00th=[   510], 90.00th=[   692], 95.00th=[   876],
     | 99.00th=[228352], 99.50th=[292864], 99.90th=[391168], 99.95th=[428032],
     | 99.99th=[501760]
   bw (  KiB/s): min=167714, max=183634, per=100.00%, avg=175624.67, stdev=785.55, samples=30
   iops        : min=1258470, max=1376786, avg=1316865.33, stdev=5793.59, samples=30
  lat (nsec)   : 250=2.39%, 500=73.15%, 750=17.48%, 1000=2.62%
  lat (usec)   : 2=0.38%, 4=0.02%, 10=0.08%, 20=0.06%, 50=0.19%
  lat (usec)   : 100=0.52%, 250=2.31%, 500=0.78%, 750=0.01%, 1000=0.01%
  lat (msec)   : 2=0.01%, 4=0.01%, 10=0.01%
  cpu          : usr=6.00%, sys=5.77%, ctx=104845, majf=0, minf=0
  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=2564400,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=1

Run status group 0 (all jobs):
   READ: bw=170MiB/s (179MB/s), 170MiB/s-170MiB/s (179MB/s-179MB/s), io=334MiB (350MB), run=1958-1958msec

Disk stats (read/write):
    dm-1: ios=68914/0, merge=0/0, ticks=13516/0, in_queue=13516, util=95.30%, aggrios=70364/0, aggrmerge=0/0, aggrticks=13712/0, aggrin_queue=13712, aggrutil=94.69%
    dm-0: ios=70364/0, merge=0/0, ticks=13712/0, in_queue=13712, util=94.69%, aggrios=70364/0, aggrmerge=0/0, aggrticks=13470/0, aggrin_queue=13469, aggrutil=94.69%
  nvme0n1: ios=70364/0, merge=0/0, ticks=13470/0, in_queue=13469, util=94.69%
#+end_src


** io_uring

Just for fun...

#+begin_src ini :tangle read-io_uring.ini
[global]
rw=randread
filename=/home/adatario/dev/tclpa/.git/annex/objects/gx/17/SHA256E-s3691765475--13300581f2404cc24774da8615a5a3d3f0adb7d68c4c8034c4fa69e727706000/SHA256E-s3691765475--13300581f2404cc24774da8615a5a3d3f0adb7d68c4c8034c4fa69e727706000
loops=100
group_reporting
thread

[job1]
ioengine=io_uring
iodepth=16
rw=randread
blocksize=4096
size=20MiB
numjobs=32
#+end_src

#+begin_src shell :exports both :results output code
  fio read-io_uring.ini
#+end_src

#+RESULTS:
#+begin_src shell
job1: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=io_uring, iodepth=16
...
fio-3.33
Starting 32 threads

job1: (groupid=0, jobs=32): err= 0: pid=42550: Fri May 19 14:20:48 2023
  read: IOPS=841k, BW=3287MiB/s (3446MB/s)(59.6GiB/18567msec)
    slat (nsec): min=330, max=12306k, avg=7989.38, stdev=38654.63
    clat (nsec): min=90, max=17003k, avg=583783.16, stdev=805140.55
     lat (nsec): min=1253, max=17004k, avg=591772.54, stdev=804599.96
    clat percentiles (nsec):
     |  1.00th=[    382],  5.00th=[   4896], 10.00th=[   7328],
     | 20.00th=[  14784], 30.00th=[  33536], 40.00th=[  98816],
     | 50.00th=[ 244736], 60.00th=[ 428032], 70.00th=[ 675840],
     | 80.00th=[1028096], 90.00th=[1695744], 95.00th=[2408448],
     | 99.00th=[3424256], 99.50th=[3784704], 99.90th=[4685824],
     | 99.95th=[5275648], 99.99th=[7569408]
   bw (  MiB/s): min= 2895, max= 3929, per=100.00%, avg=3298.97, stdev= 6.77, samples=1156
   iops        : min=741334, max=1005869, avg=844535.36, stdev=1733.23, samples=1156
  lat (nsec)   : 100=0.01%, 250=0.57%, 500=0.46%, 750=0.01%, 1000=0.01%
  lat (usec)   : 2=0.39%, 4=2.25%, 10=10.51%, 20=9.86%, 50=10.76%
  lat (usec)   : 100=5.28%, 250=10.30%, 500=12.83%, 750=9.16%, 1000=6.89%
  lat (msec)   : 2=13.16%, 4=7.24%, 10=0.32%, 20=0.01%
  cpu          : usr=3.30%, sys=8.71%, ctx=11065870, majf=0, minf=0
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.2%, 16=99.7%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.1%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=15622400,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=16

Run status group 0 (all jobs):
   READ: bw=3287MiB/s (3446MB/s), 3287MiB/s-3287MiB/s (3446MB/s-3446MB/s), io=59.6GiB (64.0GB), run=18567-18567msec

Disk stats (read/write):
    dm-1: ios=6104388/130, merge=0/0, ticks=6916752/76, in_queue=6916828, util=99.52%, aggrios=6151264/130, aggrmerge=0/0, aggrticks=6938576/76, aggrin_queue=6938652, aggrutil=99.44%
    dm-0: ios=6151264/130, merge=0/0, ticks=6938576/76, in_queue=6938652, util=99.44%, aggrios=6151264/118, aggrmerge=0/12, aggrticks=6465424/40, aggrin_queue=6465475, aggrutil=99.42%
  nvme0n1: ios=6151264/118, merge=0/12, ticks=6465424/40, in_queue=6465475, util=99.42%
#+end_src
